{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this document, we'll show how to use this package, and compare the results to a frequentist, ad-hoc method of estimating the diffusion coefficient and its uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import Bayesian_Particle_Tracking\n",
    "from Bayesian_Particle_Tracking import model\n",
    "from Bayesian_Particle_Tracking.model import log_likelihood, log_posterior, log_prior, diffusion\n",
    "from Bayesian_Particle_Tracking import io\n",
    "from Bayesian_Particle_Tracking.prior import JeffreysPrior, UniformPrior\n",
    "from Bayesian_Particle_Tracking.printable import Printable\n",
    "from Bayesian_Particle_Tracking import generate_data\n",
    "from Bayesian_Particle_Tracking.generate_data import data_generation, generator\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use generated data so that it will be convenient for evaluating the validity of our method, since we will know the diffusion coefficient. For our parameters we choose $\\sigma=10^{-8}$m, $\\mu=10^{-4}$Pa\\*s, $a=10^{-8}$m, $\\tau = 1$s, where $\\sigma$ is the measurement uncertainty on the particle's position, $\\mu$ is the dynamic viscosity of the medium, $a$ is the radius of the particle, and $\\tau$ is the time constant. the Our starting position is at the origin. We will look at a single particle over 10000 time frames or steps.\n",
    "\n",
    "Note that from our parameters we are yielded a value of D by Stokes-Einstein:\n",
    "$$D=\\frac{k_bT}{6\\pi\\mu a}=2.1973*10^{-10} m^2/s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generated the data with the following code.\n",
    "#data = Bayesian_Particle_Tracking.generate_data.generator(100000,10**(-8),10**(-4),10**(-8),[0,0,0])\n",
    "#np.save('compare_data', data)\n",
    "\n",
    "home_dir = \"/Users/alanzhou/Documents/Physics_201/final_project/\"\n",
    "data = np.load(home_dir + 'compare_data.npy')\n",
    "compare_input = diffusion(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the diffusion coefficient we will be using Markov Chain Monte Carlo with the emcee package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the model has 1 parameter; we'll use 40 walkers and 500 steps\n",
    "ndim = 1\n",
    "nwalkers = 40\n",
    "nsteps = 500\n",
    "\n",
    "#starting_positions = [abs(10**(-10) + 1e-10*np.random.randn(ndim)) for i in range(nwalkers)]\n",
    "\n",
    "starting_positions = emcee.utils.sample_ball(\n",
    "    ([10**(-10)]),\n",
    "    ([10**(-10)]),nwalkers)\n",
    "\n",
    "# set up the sampler object\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, \n",
    "                                args=[compare_input])\n",
    "# run the sampler. We use iPython's %time directive to tell us \n",
    "# how long it took (in a script, you would leave out \"%time\")\n",
    "%time sampler.run_mcmc(starting_positions, nsteps)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our walkers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax_D) = plt.subplots(1)\n",
    "ax_D.set(ylabel='D')\n",
    "for i in range(nwalkers):\n",
    "    sns.tsplot(sampler.chain[i,:,0], ax=ax_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty fast burn in time. We'll cut out the first 75 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = sampler.chain[:,75:,:]\n",
    "# reshape the samples into a 1D array where the column is D\n",
    "traces = samples.reshape(-1, ndim).T\n",
    "# create a pandas DataFrame with labels. \n",
    "parameter_samples = pd.DataFrame({'D': traces[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the marginal pdf of D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(parameter_samples['D'])\n",
    "plt.title('$D$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = parameter_samples.quantile([0.16,0.50,0.84], axis=0)\n",
    "\n",
    "print(\"D = {:.3e} + {:.3e} - {:.3e}\".format(q['D'][0.50], \n",
    "                                            q['D'][0.84]-q['D'][0.50],\n",
    "                                            q['D'][0.50]-q['D'][0.16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to what we expect, and the true value of D is contained within our 68% credibility interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad Hoc Method\n",
    "\n",
    "This method is taken from Jerome Fung's 2013 doctoral dissertation at Harvard University, _Measuring the 3D Dynamics of Multiple Colloidal Particles_. The method, described as a \"Flyvbjerg-Peterson (FB) algorithm\" is described as follows:\n",
    "\n",
    "1. Begin with correlated data (i.e., a set of squared displacements) $x_i$\n",
    "2. Block-decorrelate the data (with Flybjerg-Peterson block-decorrelation). At each block decorrelation step, calculate var($x'_i$)/(N'-1)\n",
    "3. To find the leftmost fixed-point region (smallest number of decorrelation setps), check if the variance estimate after $j$ transformations lies within the 1-sigma error bars of the variance estimate after j+1 transformations. The leftmost point to be considered fixed is the first point satisfying this criterior.\n",
    "4. Repeat Step 3 from the right to find the rightmost edge of the fixed point region.\n",
    "5. Compute a weighted average of all the var($x'_i$)/(N'-1) in the fixed-point region, where the weights by the 1-sigma error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our squared displacement function.\n",
    "\n",
    "We'll also define a function to calculate MSD for illustrative purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sq_displacement(data):\n",
    "    data_length = len(data)\n",
    "    point_before = data[:len(data)-1]\n",
    "    x_before, y_before, z_before = point_before[:,0], point_before[:,1], point_before[:,2]\n",
    "\n",
    "    data_points = data[1:]\n",
    "    x_data, y_data, z_data = data_points[:,0], data_points[:,1], data_points[:,2]\n",
    "    distance = np.sqrt((x_before-x_data)**2+(y_before-y_data)**2+(z_before-z_data)**2)\n",
    "    return distance**2\n",
    "\n",
    "def MSD(data):\n",
    "    return np.sum(sq_displacement(data))/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the results for distributions of MSDs obtained from 1000 simulated experiments in which N=1000 displacements are sampled (to generate this data the displacement array for a particle that took 1,000,000 steps  was split into 1000 arrays). The value of D is chosen with D = $2.19*10^{-10} m^2/s$ and $\\tau = 1$s such that the true value of the MSD should be \n",
    "\n",
    "$$\\langle\\Delta x^2 \\rangle = 6*D*\\tau = 1.31*10^{-9}m^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = np.split(data,1000)\n",
    "msd_array = np.array(list(map(MSD,test_data)))\n",
    "plt.hist(msd_array)\n",
    "np.median(msd_array)\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('$\\langle \\Delta x^2 \\\\rangle$ (m$^2)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of the of sample variance goes as 1/$\\sqrt{N}$, but to calculate the true standard error on the sample variance we use the FP algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "Verify that the calculations for the variance of the mean and that uncertainty on the variance is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msd_array = sq_displacement(data)\n",
    "new_length = len(msd_array)\n",
    "\n",
    "output_var = []\n",
    "var_uncertainty = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    #length of the next array should be 2 times smalller (2^(i+1) times smaller overall)\n",
    "    new_length = len(msd_array)/2\n",
    "    new_array = np.zeros(new_length)\n",
    "    #iterate over each element of the new array\n",
    "    for i in range(0,len(new_array)):\n",
    "        #Each element is given by the average of the corresponding 2 elements in the previous array\n",
    "        new_array[i] = (msd_array[2*i-1]+msd_array[2*i])/2\n",
    "    output_var.append(np.var(new_array)/(len(new_array)-1))\n",
    "    var_uncertainty.append(np.var(new_array)/(len(new_array)-1)*np.sqrt(2/(len(new_array)-1)))\n",
    "    msd_array = new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.arange(0,len(output_var))\n",
    "plt.errorbar(b,output_var, yerr = var_uncertainty, fmt='o')\n",
    "plt.ylabel('Output Variance of the Mean')\n",
    "plt.xlabel('Nuber of Block Transformations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from the FP method, we should take the average of points 3 through 5 to find the variance on the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.sqrt(np.mean(output_var[2:7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that from the FP method we obtain a 68% confidence interval (1-sigma) of $\\pm 1.8*10^{-11}$\n",
    "\n",
    "Compare this to the 68% credibility interval by MCMC of $~ \\pm 9*10^{-12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CGW Method\n",
    "\n",
    "This method is alluded to in Jerome Fung's Thesis, and is one of the primary ways of estimating the diffusion coefficient in co The idea is to take a set of MSDs and then fit a line of the form 2D$\\tau$ to the plot of MSD as a function of lag time. The relative standard error of the variance, $\\sigma_{var}/var$ can be computed by:\n",
    "\n",
    "$$\\frac{\\sigma_{var}}{var}=\\sqrt{\\frac{2}{N_{ind}-1}}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$N_{ind, CGW} = 2\\frac{N-n}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
